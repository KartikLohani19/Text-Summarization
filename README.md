The project entails the summarization of an input using a seq2seqtransformer. 
This is done using the libraries such as torch, torchdata, transformers and data sets.
The model uses huggingface to get the corpus and uses FLAN-T5 language model to train the LLM. 
The model uses incontext learning to get the otimum output.
